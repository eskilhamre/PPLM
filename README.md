# PPLM
Plug and Play Language Model implementation. Allows to steer topic and attributes of generated text of GPT-2 models.

# Introduction
This contains the code of my final project for a graduate-level Natural language processing course at UiB (course code INF368A, Fall 2021). It's just a simplefied version of the original [PPLM repo](https://github.com/uber-research/PPLM), containing all the necessary code to steer language generation in a simple manner. I've added another discriminator model trained on the AG news dataset. With this, the technology can be used to steer language generation into any of the following topics: **global, sports, business and sci/tech.**  

# Technologies
- PyTorch
- Transformers
- Jupyter Notebook

# Terminology: The basics
This might come in handy when studying the code in *main.ipynb* and the generated text.
- The main feature of PPLM is the use of *discriminators* in conjunction with transformer language models for text generation. A discriminator is simply a linear classifier trained to distinguish different classes of text - for instance positive and negative sentiment.
- We refer to text generated by the help of a discriminator as *perturbed text*. Conversely, text generated by the use of the GPT language model alone is referred to as *unperturbed text*.

# Added Content
- **main.ipynb**: My own code. Contains the code for both training the news discriminator and for generating text samples.
- **generation_results/**:
    - **all-samples.json**: The generated text samples, both unperturbed and perturbed using each news class.
    - **all-samples-structured.json**: Same data as above, but structured wrt. the text prefixes given to the language model.
    - **comparisons.json**: Contains only the "best" generated samples for each news class and text prefix. Illustrates how much more *on topic* the perturbed text are.
- **discrim_models/**:
    - **news_classifierhead.pt**: The PyTorch discriminator model.

# How to run
To be frank, this is an experiment rather than a technology/tool. To rerun my experiment, run the cells in the *main.ipynb* file in a Google Colab environment (or something equivalent).   
**WARNING**: I ran into some version inconsistency troubles when trying to set up an environment locally due to the PPLM code being somewhat outdated, so I really advice you to use Google Colab for this.

# Acknowledgments
All credits goes to the Uber AI research team. Here are both their [code](https://github.com/uber-research/PPLM) and their [research paper](https://arxiv.org/abs/1912.02164).
